{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化后的 LSTM-AdaBoost 时间序列预测模型\n",
    "\n",
    "## 1. 模型介绍\n",
    "本项目实现了一个基于**LSTM (长短期记忆网络)**和**AdaBoost (自适应增强)**的集成学习模型，用于时间序列预测。\n",
    "\n",
    "- **LSTM**: 作为模型的“弱学习器”，擅长捕捉时间序列中的长期依赖关系。\n",
    "- **AdaBoost**: 作为集成框架，通过串行训练多个LSTM，让后续模型更关注先前模型预测错误的样本，从而逐步提升整体模型的预测精度和鲁棒性。\n",
    "\n",
    "## 2. 核心优化点\n",
    "此版本相比原始代码，进行了以下关键优化：\n",
    "\n",
    "1.  **修正数据划分**: 采用严格的**时间顺序**划分训练、验证和测试集，杜绝了未来数据泄露，保证了模型评估的有效性。\n",
    "2.  **修正数据标准化**: 标准化器 (Scaler) **仅在训练集上拟合**，然后应用于验证集和测试集，避免了数据泄露。\n",
    "3.  **增强模型封装**: 添加了统一的 `save` 和 `load` 方法，可以一键保存和加载模型的完整状态（包括所有弱学习器、标准化器和权重），极大地方便了模型的部署和复用。\n",
    "4.  **提升代码可读性**: 优化了代码结构，并增加了详细的中文注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import json\n",
    "import joblib # 用于保存scaler\n",
    "\n",
    "# 设置日志级别，只显示错误信息\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTMAdaBoost 模型类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAdaBoost:\n",
    "    def __init__(self, n_estimators=10, seq_length=24, learning_rate=0.001, use_gpu=True,\n",
    "                 early_stopping_patience=5, reduce_lr_patience=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "        self.estimator_weights = []\n",
    "        self.scaler_X = StandardScaler() # 特征标准化器\n",
    "        self.scaler_y = StandardScaler() # 目标值标准化器\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.reduce_lr_patience = reduce_lr_patience\n",
    "        self.is_fitted = False # 标记模型是否已训练\n",
    "\n",
    "        if use_gpu:\n",
    "            self._configure_gpu()\n",
    "\n",
    "    def _configure_gpu(self):\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                print(f\"检测到 {len(gpus)} 个GPU，已配置内存增长模式。\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"GPU配置失败: {e}\")\n",
    "        else:\n",
    "            print(\"未检测到GPU，将使用CPU进行训练。\")\n",
    "\n",
    "    def _create_sequences(self, X, y):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(len(X) - self.seq_length):\n",
    "            Xs.append(X[i:(i + self.seq_length)])\n",
    "            ys.append(y[i + self.seq_length])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    def _build_lstm_model(self, input_shape):\n",
    "        model = Sequential([\n",
    "            LSTM(32, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(0.3),\n",
    "            LSTM(16),\n",
    "            Dropout(0.3),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate), metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32, verbose=1):\n",
    "        \"\"\"使用修正后的数据划分和标准化流程来训练模型\"\"\"\n",
    "        # [优化] 仅在训练数据上拟合标准化器\n",
    "        X_train_scaled = self.scaler_X.fit_transform(X_train)\n",
    "        y_train_scaled = self.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # [优化] 对验证数据使用相同的标准化器进行转换\n",
    "        X_val_scaled = self.scaler_X.transform(X_val)\n",
    "        y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # 创建时间序列数据\n",
    "        X_train_seq, y_train_seq = self._create_sequences(X_train_scaled, y_train_scaled)\n",
    "        X_val_seq, y_val_seq = self._create_sequences(X_val_scaled, y_val_scaled)\n",
    "\n",
    "        if X_train_seq.shape[0] == 0:\n",
    "            raise ValueError(\"训练数据太少，无法创建任何时间序列样本。请增加训练数据量或减小seq_length。\")\n",
    "\n",
    "        # 初始化样本权重\n",
    "        sample_weights = np.ones(len(X_train_seq)) / len(X_train_seq)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            print(f\"\\n--- 训练第 {i+1}/{self.n_estimators} 个LSTM模型 ---\")\n",
    "            model = self._build_lstm_model((self.seq_length, X_train.shape[1]))\n",
    "\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=self.early_stopping_patience, restore_best_weights=True, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=self.reduce_lr_patience, min_lr=1e-6, verbose=1)\n",
    "            ]\n",
    "\n",
    "            history = model.fit(X_train_seq, y_train_seq,\n",
    "                              epochs=epochs,\n",
    "                              batch_size=batch_size,\n",
    "                              validation_data=(X_val_seq, y_val_seq),\n",
    "                              verbose=verbose,\n",
    "                              sample_weight=sample_weights,\n",
    "                              callbacks=callbacks)\n",
    "\n",
    "            y_pred_train = model.predict(X_train_seq).flatten()\n",
    "            error = np.abs(y_pred_train - y_train_seq)\n",
    "            weighted_error = np.sum(sample_weights * error)\n",
    "\n",
    "            # 防止除零错误\n",
    "            if weighted_error >= 1.0:\n",
    "                weighted_error = 1.0 - 1e-10\n",
    "\n",
    "            alpha = 0.5 * np.log((1 - weighted_error) / max(weighted_error, 1e-10))\n",
    "            error_rate = error / (np.max(error) + 1e-10)\n",
    "            sample_weights *= np.exp(alpha * error_rate)\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "            self.estimators.append(model)\n",
    "            self.estimator_weights.append(alpha)\n",
    "            print(f\"模型 {i+1} 训练完成: 误差率={weighted_error:.4f}, 学习器权重={alpha:.4f}\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"模型尚未训练，请先调用fit()方法。\")\n",
    "        \n",
    "        X_scaled = self.scaler_X.transform(X)\n",
    "        # 创建序列时，y可以是任意值，因为它不会被使用\n",
    "        X_seq, _ = self._create_sequences(X_scaled, np.zeros(len(X_scaled)))\n",
    "        \n",
    "        if X_seq.shape[0] == 0:\n",
    "            return np.array([]) # 如果输入数据不足以形成一个序列，返回空数组\n",
    "\n",
    "        weighted_predictions = np.zeros(len(X_seq))\n",
    "        total_weight = sum(self.estimator_weights)\n",
    "\n",
    "        for weight, model in zip(self.estimator_weights, self.estimators):\n",
    "            pred = model.predict(X_seq).flatten()\n",
    "            weighted_predictions += weight * pred\n",
    "\n",
    "        # 计算加权平均\n",
    "        if total_weight > 0:\n",
    "            final_predictions_scaled = weighted_predictions / total_weight\n",
    "        else:\n",
    "            final_predictions_scaled = weighted_predictions # 如果所有权重为0，则为简单平均\n",
    "\n",
    "        # 反标准化\n",
    "        final_predictions = self.scaler_y.inverse_transform(final_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "        return final_predictions\n",
    "    \n",
    "    # [优化] 添加统一的保存方法\n",
    "    def save(self, directory_path):\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "        \n",
    "        # 保存弱学习器\n",
    "        for i, model in enumerate(self.estimators):\n",
    "            model.save(os.path.join(directory_path, f'estimator_{i}.h5'))\n",
    "        \n",
    "        # 保存标准化器\n",
    "        joblib.dump(self.scaler_X, os.path.join(directory_path, 'scaler_X.gz'))\n",
    "        joblib.dump(self.scaler_y, os.path.join(directory_path, 'scaler_y.gz'))\n",
    "        \n",
    "        # 保存模型配置和权重\n",
    "        config = {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'seq_length': self.seq_length,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'estimator_weights': self.estimator_weights,\n",
    "            'is_fitted': self.is_fitted\n",
    "        }\n",
    "        with open(os.path.join(directory_path, 'config.json'), 'w') as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "            \n",
    "        print(f\"模型已完整保存至: {directory_path}\")\n",
    "\n",
    "    # [优化] 添加统一的加载方法\n",
    "    @classmethod\n",
    "    def load(cls, directory_path):\n",
    "        config_path = os.path.join(directory_path, 'config.json')\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # 使用保存的配置初始化模型\n",
    "        model_instance = cls(\n",
    "            n_estimators=config['n_estimators'],\n",
    "            seq_length=config['seq_length'],\n",
    "            learning_rate=config['learning_rate']\n",
    "        )\n",
    "        model_instance.estimator_weights = config['estimator_weights']\n",
    "        model_instance.is_fitted = config['is_fitted']\n",
    "        \n",
    "        # 加载标准化器\n",
    "        model_instance.scaler_X = joblib.load(os.path.join(directory_path, 'scaler_X.gz'))\n",
    "        model_instance.scaler_y = joblib.load(os.path.join(directory_path, 'scaler_y.gz'))\n",
    "        \n",
    "        # 加载弱学习器\n",
    "        model_instance.estimators = []\n",
    "        for i in range(config['n_estimators']):\n",
    "            estimator_path = os.path.join(directory_path, f'estimator_{i}.h5')\n",
    "            estimator = load_model(estimator_path)\n",
    "            model_instance.estimators.append(estimator)\n",
    "            \n",
    "        print(f\"模型已从 {directory_path} 完整加载。\")\n",
    "        return model_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 辅助函数（数据加载、评估、可视化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(file_path, train_ratio=0.7, val_ratio=0.15):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if df.isnull().any().any():\n",
    "        print(\"检测到缺失值，使用前向填充(ffill)处理。\")\n",
    "        df = df.fillna(method='ffill')\n",
    "    \n",
    "    X = df.drop(['value'], axis=1).values\n",
    "    y = df['value'].values\n",
    "    \n",
    "    # [优化] 按时间顺序划分数据集\n",
    "    train_size = int(len(X) * train_ratio)\n",
    "    val_size = int(len(X) * val_ratio)\n",
    "    \n",
    "    X_train, y_train = X[:train_size], y[:train_size]\n",
    "    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "    X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "    \n",
    "    print(f\"数据加载与划分完成:\")\n",
    "    print(f\"  - 训练集: {len(X_train)} 条\")\n",
    "    print(f\"  - 验证集: {len(X_val)} 条\")\n",
    "    print(f\"  - 测试集: {len(X_test)} 条\")\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100 # 防止除以0\n",
    "    \n",
    "    print(\"\\n--- 模型评估结果 ---\")\n",
    "    print(f\"均方误差 (MSE): {mse:.4f}\")\n",
    "    print(f\"平均绝对误差 (MAE): {mae:.4f}\")\n",
    "    print(f\"R-squared (R²): {r2:.4f}\")\n",
    "    print(f\"平均绝对百分比误差 (MAPE): {mape:.2f}%\")\n",
    "    \n",
    "    return {'mse': mse, 'mae': mae, 'r2': r2, 'mape': mape}\n",
    "\n",
    "def plot_predictions(y_true, y_pred, title=\"模型预测结果\", save_path=None):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei'] # 设置字体以显示中文\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # 绘制整体对比图\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(y_true, label='真实值', color='blue', alpha=0.8)\n",
    "    plt.plot(y_pred, label='预测值', color='red', linestyle='--')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('时间步')\n",
    "    plt.ylabel('负荷值')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # [可视化要求] 绘制前100个点的细节对比图\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sample_size = min(100, len(y_true))\n",
    "    plt.plot(y_true[:sample_size], 'o-', label='真实值', color='blue')\n",
    "    plt.plot(y_pred[:sample_size], 'x--', label='预测值', color='red')\n",
    "    plt.title(f'预测细节（前 {sample_size} 个数据点）', fontsize=14)\n",
    "    plt.xlabel('时间步')\n",
    "    plt.ylabel('负荷值')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"图表已保存至: {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 主执行流程\n",
    "在这里我们将执行完整的模型训练、评估、保存、加载和预测流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 参数配置与数据加载 ---\n",
    "file_path = '../../code/new_data.csv' # 请确保文件路径正确\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_and_split_data(file_path)\n",
    "\n",
    "# --- 2. 模型训练 ---\n",
    "model = LSTMAdaBoost(\n",
    "    n_estimators=5,       # [建议] 先用较小的数量（如3-5）快速迭代，找到最优参数后再增加\n",
    "    seq_length=48,        # [建议] 序列长度可以根据数据特性调整，例如24(一天)或168(一周)\n",
    "    use_gpu=True,\n",
    "    early_stopping_patience=10,\n",
    "    reduce_lr_patience=5\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, X_val, y_val, epochs=100, batch_size=64)\n",
    "\n",
    "# --- 3. 在测试集上评估 ---\n",
    "# 注意：y_test需要根据seq_length进行裁剪以匹配预测输出的长度\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_true_test_aligned = y_test[model.seq_length:]\n",
    "evaluate_performance(y_true_test_aligned, y_pred_test)\n",
    "\n",
    "# --- 4. 可视化预测结果 ---\n",
    "plot_predictions(y_true_test_aligned, y_pred_test, save_path='./optimized_prediction_plot.png')\n",
    "\n",
    "# --- 5. 保存和加载模型演示 ---\n",
    "model_save_dir = './final_lstm_adaboost_model'\n",
    "model.save(model_save_dir)\n",
    "\n",
    "# 加载模型\n",
    "loaded_model = LSTMAdaBoost.load(model_save_dir)\n",
    "\n",
    "# 使用加载后的模型进行预测，验证其功能是否正常\n",
    "y_pred_from_loaded_model = loaded_model.predict(X_test)\n",
    "\n",
    "# 比较两次预测结果是否一致\n",
    "if np.allclose(y_pred_test, y_pred_from_loaded_model):\n",
    "    print(\"\\n加载的模型预测结果与原模型一致，保存和加载功能验证成功！\")\n",
    "else:\n",
    "    print(\"\\n警告：加载的模型预测结果与原模型不一致！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}